{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be6cdaf0aea1464e",
   "metadata": {},
   "source": [
    "## Model Comparison & Method Selection\n",
    "\n",
    "## Purpose\n",
    "This notebook documents our experimental process for selecting optimal preprocessing methods and classification. Results from this notebook will inform final implementation decisions in the main analysis notebook.\n",
    "\n",
    "**Note**: This is an experimental notebook. Only winning approaches will be implemented in the production notebook [sentiment_analysis](/notebooks/sentiment_analysis.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "id": "8166e55a51d1633c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T10:46:06.804471Z",
     "start_time": "2025-12-08T10:46:06.564677Z"
    }
   },
   "source": [
    "# Exam the dataset for normalization decision\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load dataset\n",
    "df_check = pd.read_csv('../data/customer_sentiment.csv')\n",
    "\n",
    "# Sample review texts\n",
    "print(\"\\n1. SAMPLE REVIEW TEXTS (first 10):\")\n",
    "for i, review in enumerate(df_check['review_text'].head(10), 1):\n",
    "    print(f\"\\n   {i}. {review[:]}\")\n",
    "\n",
    "# Platform distribution\n",
    "print(\"\\n2. PLATFORM DISTRIBUTION:\")\n",
    "print(df_check['platform'].value_counts())\n",
    "\n",
    "# Get all words from reviews (lowercased)\n",
    "all_words = ' '.join(df_check['review_text'].str.lower()).split()\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "print(\"\\n3. TOP 30 MOST COMMON WORDS IN REVIEWS:\")\n",
    "for word, count in word_counts.most_common(50):\n",
    "    print(f\"   {word.ljust(20)} {count}\")\n",
    "\n",
    "# Check for retail/delivery terms\n",
    "retail_terms = ['delivery', 'deliver', 'delivered', 'shipping', 'shipped', 'ship',\n",
    "                'refund', 'return', 'returned', 'money', 'back',\n",
    "                'order', 'ordered', 'ordering', 'package', 'packaging',\n",
    "                'quality', 'product', 'service', 'customer']\n",
    "\n",
    "print(\"\\n4. FREQUENCY OF RETAIL/DELIVERY TERMS:\")\n",
    "for term in retail_terms:\n",
    "    count = sum(1 for review in df_check['review_text'].str.lower() if term in review)\n",
    "    if count > 0:\n",
    "        print(f\"   {term.ljust(15)} appears in {count} reviews ({count / len(df_check) * 100:.1f}%)\")\n",
    "\n",
    "# 6. Check platform mentions in text\n",
    "print(\"\\n5. PLATFORM NAMES MENTIONED IN REVIEWS:\")\n",
    "platforms = df_check['platform'].unique()\n",
    "for platform in platforms[:]:  # Check first 10 platforms\n",
    "    count = sum(1 for review in df_check['review_text'].str.lower() if platform.lower() in review)\n",
    "    if count > 0:\n",
    "        print(f\"   {platform.ljust(20)}: mentioned {count} times\")\n",
    "print(\"not mentioned\")\n",
    "\n",
    "# Conclusion\n",
    "# No platforms mentioned in text - platform normalization unnecessary\n",
    "# Terms are already simple - \"delivery\", \"quality\", \"product\" are base forms\n",
    "# Lemmatization could be considered for words like \"delivered\" -> \"deliver\",\n",
    "# \"ordered\" -> \"order\" \"packing\" -> \"packaging\" -> \"package\"\n",
    "# Reviews are short - complex normalization adds small value\n",
    "# Decision: (clean + lemmatize + stopwords)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. SAMPLE REVIEW TEXTS (first 10):\n",
      "\n",
      "   1. very disappointed with the quality.\n",
      "\n",
      "   2. fast delivery and great packaging.\n",
      "\n",
      "   3. very disappointed with the quality.\n",
      "\n",
      "   4. product stopped working after few days.\n",
      "\n",
      "   5. neutral about the quality.\n",
      "\n",
      "   6. amazing experience, highly recommend!\n",
      "\n",
      "   7. great value for money.\n",
      "\n",
      "   8. excellent product! exceeded expectations.\n",
      "\n",
      "   9. product is okay, nothing special.\n",
      "\n",
      "   10. great value for money.\n",
      "\n",
      "2. PLATFORM DISTRIBUTION:\n",
      "platform\n",
      "nykaa                   1301\n",
      "snapdeal                1289\n",
      "others                  1286\n",
      "reliance digital        1279\n",
      "zepto                   1278\n",
      "facebook marketplace    1272\n",
      "paytm mall              1271\n",
      "myntra                  1267\n",
      "croma                   1266\n",
      "flipkart                1264\n",
      "boat                    1257\n",
      "lenskart                1241\n",
      "jiomart                 1240\n",
      "meesho                  1240\n",
      "ajio                    1234\n",
      "bigbasket               1230\n",
      "shopclues               1220\n",
      "tata cliq               1201\n",
      "swiggy instamart        1192\n",
      "amazon                  1172\n",
      "Name: count, dtype: int64\n",
      "\n",
      "3. TOP 30 MOST COMMON WORDS IN REVIEWS:\n",
      "   the                  6978\n",
      "   delivery             5023\n",
      "   quality.             4952\n",
      "   and                  3992\n",
      "   packaging.           3992\n",
      "   product              3972\n",
      "   very                 3932\n",
      "   with                 3932\n",
      "   great                3919\n",
      "   was                  3038\n",
      "   amazing              2109\n",
      "   experience,          2109\n",
      "   highly               2109\n",
      "   recommend!           2109\n",
      "   is                   2064\n",
      "   late                 2044\n",
      "   poor                 2044\n",
      "   not                  2026\n",
      "   worth                2026\n",
      "   price.               2026\n",
      "   customer             2007\n",
      "   service              2007\n",
      "   unhelpful.           2007\n",
      "   satisfied            1980\n",
      "   value                1971\n",
      "   for                  1971\n",
      "   money.               1971\n",
      "   excellent            1970\n",
      "   product!             1970\n",
      "   exceeded             1970\n",
      "   expectations.        1970\n",
      "   disappointed         1952\n",
      "   fast                 1948\n",
      "   stopped              1908\n",
      "   working              1908\n",
      "   after                1908\n",
      "   few                  1908\n",
      "   days.                1908\n",
      "   okay,                1033\n",
      "   nothing              1033\n",
      "   special.             1033\n",
      "   fine,                1031\n",
      "   decent.              1031\n",
      "   neutral              1020\n",
      "   about                1020\n",
      "   works                1018\n",
      "   fine                 1018\n",
      "   but                  1018\n",
      "   could                1018\n",
      "   be                   1018\n",
      "\n",
      "4. FREQUENCY OF RETAIL/DELIVERY TERMS:\n",
      "   delivery        appears in 5023 reviews (20.1%)\n",
      "   deliver         appears in 5023 reviews (20.1%)\n",
      "   money           appears in 1971 reviews (7.9%)\n",
      "   packaging       appears in 3992 reviews (16.0%)\n",
      "   quality         appears in 4952 reviews (19.8%)\n",
      "   product         appears in 5942 reviews (23.8%)\n",
      "   service         appears in 2007 reviews (8.0%)\n",
      "   customer        appears in 2007 reviews (8.0%)\n",
      "\n",
      "5. PLATFORM NAMES MENTIONED IN REVIEWS:\n",
      "not mentioned\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## spaCy vs NLTK Lemmatization Comparison",
   "id": "1f18240d17d5d00d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T11:15:43.875303Z",
     "start_time": "2025-12-08T11:14:14.084119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Download spaCy model (silent)\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print(\"spaCy model loaded successfully!\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    subprocess.run(\n",
    "        [sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL\n",
    "    )\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print(\"spaCy model downloaded successfully!\")\n",
    "\n",
    "# Download NLTK resources (silent)\n",
    "nltk_resources = ['punkt', 'punkt_tab', 'stopwords', 'wordnet', 'omw-1.4']\n",
    "for resource in nltk_resources:\n",
    "    nltk.download(resource, quiet=True)\n",
    "print(\"NLTK resources ready!\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('../data/customer_sentiment.csv')\n",
    "print(f\"Dataset loaded: {len(df):,} reviews\")\n",
    "\n",
    "# Initialize tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Text Cleaning Function\n",
    "def clean_text(text):\n",
    "    \"\"\" Clean and Normalize Text Data\n",
    "     Steps:\n",
    "      1. Lowercase the text\n",
    "      2. Remove URLs\n",
    "      3. Remove emails addresses\n",
    "      4. Remove punctuation\n",
    "      5. Remove extra whitespace\n",
    "     \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Tokenization Function\n",
    "def tokenize_text(text):\n",
    "    \"\"\" Tokenize text into words \"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Stopword Removal Function\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "     Remove common English stopwords, but keep negations.\n",
    "     Like: not, no, n't, don't, doesn't, didn't, never, none, nobody, nothing, neither, nor\n",
    "     Negations are crucial for sentiment analysis.\n",
    "     \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    negations = {\"not\", \"no\", \"n't\", \"don't\", \"doesn't\", \"didn't\",\n",
    "                 \"never\", \"none\", \"nobody\", \"nothing\", \"neither\", \"nor\"}\n",
    "    return [words for words in tokens if words not in stop_words or words in negations]\n",
    "\n",
    "# Lemmatization Functions\n",
    "def lemmatize_tokens_nltk(tokens):\n",
    "    \"\"\" Lemmatize text using NLTK's WordNetLemmatizer\n",
    "    Example:\n",
    "        packages -> package\n",
    "        delivered -> deliver\n",
    "    \"\"\"\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def lemmatize_tokens_spacy(tokens):\n",
    "    \"\"\" Lemmatize text using spaCy\n",
    "    Example:\n",
    "      running -> run, better -> good | While NLTK may not handle these well\n",
    "    \"\"\"\n",
    "    text = ' '.join(tokens)\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "def preprocess_pipeline(text, method='nltk'):\n",
    "    \"\"\" Complete Preprocessing Pipeline\n",
    "     method: 'nltk' or 'spacy' for lemmatization\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "\n",
    "    if method == 'nltk':\n",
    "        tokens = lemmatize_tokens_nltk(tokens)\n",
    "    elif method == 'spacy':\n",
    "        tokens = lemmatize_tokens_spacy(tokens)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'nltk' or 'spacy'\")\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Sample Comparison\n",
    "print(\"\\nPreprocessing Comparison: NLTK vs spaCy (Sample)\")\n",
    "sample_size = 5\n",
    "for i in range(sample_size):\n",
    "    original = df.loc[i, 'review_text']\n",
    "    nltk_result = preprocess_pipeline(original)\n",
    "    spacy_result = preprocess_pipeline(original, method='spacy')\n",
    "\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"NLTK:     {nltk_result}\")\n",
    "    print(f\"spaCy:    {spacy_result}\")\n",
    "\n",
    "# Execution Time Test\n",
    "print(f\"\\nExecution Time Comparison - Processing {len(df):,} reviews\")\n",
    "\n",
    "# NLTK timing\n",
    "print(\"Testing NLTK...\")\n",
    "start_time = time.time()\n",
    "df['processed_text_nltk'] = df['review_text'].apply(preprocess_pipeline)\n",
    "nltk_time = time.time() - start_time\n",
    "print(f\"NLTK Lemmatization: {nltk_time:.2f} seconds\")\n",
    "\n",
    "# spaCy timing\n",
    "print(\"Testing spaCy...\")\n",
    "start_time = time.time()\n",
    "df['processed_text_spacy'] = df['review_text'].apply(lambda x: preprocess_pipeline(x, method='spacy'))\n",
    "spacy_time = time.time() - start_time\n",
    "print(f\"spaCy Lemmatization: {spacy_time:.2f} seconds\")\n",
    "\n",
    "# Speed comparison\n",
    "print(f\"\\nSpeed Comparison:\")\n",
    "print(f\"spaCy is {spacy_time/nltk_time:.2f}x slower than NLTK\")\n",
    "print(f\"Time difference: {spacy_time - nltk_time:.2f} seconds\")"
   ],
   "id": "9cd9b52064544827",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model loaded successfully!\n",
      "NLTK resources ready!\n",
      "Dataset loaded: 25,000 reviews\n",
      "\n",
      "Preprocessing Comparison: NLTK vs spaCy (Sample)\n",
      "\n",
      "Sample 1:\n",
      "Original: very disappointed with the quality.\n",
      "NLTK:     disappointed quality\n",
      "spaCy:    disappoint quality\n",
      "\n",
      "Sample 2:\n",
      "Original: fast delivery and great packaging.\n",
      "NLTK:     fast delivery great packaging\n",
      "spaCy:    fast delivery great packaging\n",
      "\n",
      "Sample 3:\n",
      "Original: very disappointed with the quality.\n",
      "NLTK:     disappointed quality\n",
      "spaCy:    disappoint quality\n",
      "\n",
      "Sample 4:\n",
      "Original: product stopped working after few days.\n",
      "NLTK:     product stopped working day\n",
      "spaCy:    product stop work day\n",
      "\n",
      "Sample 5:\n",
      "Original: neutral about the quality.\n",
      "NLTK:     neutral quality\n",
      "spaCy:    neutral quality\n",
      "\n",
      "Execution Time Comparison - Processing 25,000 reviews\n",
      "Testing NLTK...\n",
      "NLTK Lemmatization: 3.76 seconds\n",
      "Testing spaCy...\n",
      "spaCy Lemmatization: 85.54 seconds\n",
      "\n",
      "Speed Comparison:\n",
      "spaCy is 22.72x slower than NLTK\n",
      "Time difference: 81.78 seconds\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ">We observe that while spaCy provides etter verb normalization,smaller vocabularies. However, it is more aggressive and it is 22.72x slower than NLTK. Given our large dataset size **(25,000 reviews)** and the relatively minor differences in lemmatization for our specific use case, we will proceed with NLTK for lemmatization in our final preprocessing pipeline.",
   "id": "1f0ad92b78b554b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
