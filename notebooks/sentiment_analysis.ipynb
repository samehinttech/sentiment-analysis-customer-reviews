{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"## Part 1: Libraries Import \",\n",
    "   \"id\": \"ca6211f4c42be795\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"### 1.1 Libraries for Data Analysis and Visualization\",\n",
    "   \"id\": \"7f924d757a38e734\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"# Data Analysis and Visualization Libraries\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display settings\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')# To keep the output clean from warnings, can be removed after demonstration.\\n\",\n",
    "    \"pd.set_option('display.max_columns', None) # Display all columns\\n\",\n",
    "    \"sns.set_theme(style=\\\"whitegrid\\\")\"\n",
    "   ],\n",
    "   \"id\": \"8e190f480331f250\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"### 1.2 Libraries for NLP and Modeling\",\n",
    "   \"id\": \"646189baaf646560\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"# Text Preprocessing\\n\",\n",
    "    \"import re\\n\",\n",
    "    \"import string\\n\",\n",
    "    \"import nltk\\n\",\n",
    "    \"from nltk.corpus import stopwords\\n\",\n",
    "    \"from nltk.stem import WordNetLemmatizer\\n\",\n",
    "    \"from nltk.tokenize import word_tokenize\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Feature Extraction\\n\",\n",
    "    \"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ML Models\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split\\n\",\n",
    "    \"from sklearn.naive_bayes import MultinomialNB\\n\",\n",
    "    \"from sklearn.linear_model import LogisticRegression\\n\",\n",
    "    \"from sklearn.pipeline import Pipeline\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Model Evaluation\\n\",\n",
    "    \"from sklearn.metrics import (\\n\",\n",
    "    \"    accuracy_score,\\n\",\n",
    "    \"    precision_recall_fscore_support,\\n\",\n",
    "    \"    classification_report,\\n\",\n",
    "    \"    confusion_matrix,\\n\",\n",
    "    \"    ConfusionMatrixDisplay\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Baseline Sentiment Analysis\\n\",\n",
    "    \"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Transformers\\n\",\n",
    "    \"from transformers import pipeline as hf_pipeline  # Renamed to avoid conflict\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Topic Modeling\\n\",\n",
    "    \"from sklearn.decomposition import LatentDirichletAllocation\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Utilities\\n\",\n",
    "    \"from collections import Counter\\n\",\n",
    "    \"import time\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Download NLTK resources (silent)\\n\",\n",
    "    \"nltk_resources = ['punkt', 'punkt_tab', 'stopwords', 'wordnet', 'omw-1.4']\\n\",\n",
    "    \"for resource in nltk_resources:\\n\",\n",
    "    \"    nltk.download(resource, quiet=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"NLTK resources ready!\\\")\\n\",\n",
    "    \"print(\\\"All libraries imported successfully!\\\")\"\n",
    "   ],\n",
    "   \"id\": \"b48f0afd49bbe657\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"### 1.3 Reading Data\",\n",
    "   \"id\": \"ae903c2ea31b7eb0\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"df = pd.read_csv('../data/customer_sentiment.csv')\\n\",\n",
    "    \"# Display basic information\\n\",\n",
    "    \"print(f\\\"Shape: {df.shape[0]} rows x {df.shape[1]} columns\\\")\\n\",\n",
    "    \"print(f\\\"\\\\nData Attributes:\\\\n{list(df.columns)}\\\")\"\n",
    "   ],\n",
    "   \"id\": \"42174e5e1e2776d7\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"## Part 2: Exploratory Data Analysis (EDA)\",\n",
    "   \"id\": \"a377ce5bb9cefcb6\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": [\n",
    "    \"### 2.1 EDA - Part 1: Basic Exploration and Summary Statistics\\n\",\n",
    "    \"1. Display first 5 rows of the dataset\\n\",\n",
    "    \"2. Display dataset information\\n\",\n",
    "    \"3. Display basic statistics for numerical columns\\n\",\n",
    "    \"4. Check for missing values\"\n",
    "   ],\n",
    "   \"id\": \"24814604d2b243ad\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"# data_overview: List of tuples containing (section title, display function)\\n\",\n",
    "    \"data_overview = [\\n\",\n",
    "    \"    (\\\"FIRST 5 ROWS\\\", lambda: display(df.head())),\\n\",\n",
    "    \"    (\\\"DATASET INFO\\\", lambda: df.info()),\\n\",\n",
    "    \"    (\\\"STATISTICS SUMMARY\\\", lambda: display(df.describe())),\\n\",\n",
    "    \"    (\\\"MISSING VALUES\\\", lambda: print(df.isnull().sum()[df.isnull().sum() > 0]\\n\",\n",
    "    \"                                     if df.isnull().any().any() else \\\"No missing values found.\\\"))]\\n\",\n",
    "    \"# Display each section with numbered headers\\n\",\n",
    "    \"# 1 is tuple's elements index, to start the headers number from 1\\n\",\n",
    "    \"for i, (title, func) in enumerate(data_overview, 1):\\n\",\n",
    "    \"    print(f\\\"\\\\n{i} - {title}:\\\")\\n\",\n",
    "    \"    # call the lambda function to display the content\\n\",\n",
    "    \"    func()\"\n",
    "   ],\n",
    "   \"id\": \"d10af59e9301401d\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"### 2.2 Basic Data Quality Checks\",\n",
    "   \"id\": \"9bf210774e3589c9\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"# Function to check for duplicates\\n\",\n",
    "    \"def check_duplicates(dataframe):\\n\",\n",
    "    \"    duplicate_count = dataframe.duplicated().sum()\\n\",\n",
    "    \"    print(f\\\"Total Duplicates: {duplicate_count}\\\")\\n\",\n",
    "    \"    print(f\\\"{'Warnings: Duplicates found!' if duplicate_count > 0 else 'No duplicates found.'}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# function to check unique values in categorical columns\\n\",\n",
    "    \"def check_unique_values(dataframe):\\n\",\n",
    "    \"    categorical_attrs = dataframe.select_dtypes(include=['object', 'category']).columns\\n\",\n",
    "    \"    for cols in categorical_attrs:\\n\",\n",
    "    \"        print(f\\\"\\\\nColumn:[{cols}] has {dataframe[cols].nunique()} unique values.\\\")\\n\",\n",
    "    \"        print(f\\\"Values: {list(dataframe[cols].unique())}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Sum of sentiment distribution. The sentiment col. will be ignored during modeling,\\n\",\n",
    "    \"# but we decided to keep it for EDA purposes and use it later for performance evaluation.\\n\",\n",
    "    \"def sentiment_distribution(dataframe):\\n\",\n",
    "    \"    \\\"\\\"\\\" Display sentiment distribution \\\"\\\"\\\"\\n\",\n",
    "    \"    print(dataframe['sentiment'].value_counts())\\n\",\n",
    "    \"    print(\\\"\\\\nPercentages:\\\",\\n\",\n",
    "    \"          dataframe['sentiment'].value_counts(normalize=True).round(3) * 100)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# List of a tuples to hold the checks to be performed\\n\",\n",
    "    \"data_quality_checks = [\\n\",\n",
    "    \"    (\\\"DUPLICATE CHECK\\\", check_duplicates),\\n\",\n",
    "    \"    (\\\"UNIQUE VALUES IN CATEGORICAL COLUMNS\\\", check_unique_values),\\n\",\n",
    "    \"    (\\\"CUSTOMER RATING DISTRIBUTION\\\", lambda dataframe:print(dataframe['customer_rating'].value_counts().sort_index())),\\n\",\n",
    "    \"    (\\\"SENTIMENT DISTRIBUTION\\\", sentiment_distribution)]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Perform each data quality check with numbered headers\\n\",\n",
    "    \"for i , (check_title, check_func) in enumerate(data_quality_checks, 1):\\n\",\n",
    "    \"    print(f\\\"\\\\n{i} - {check_title}:\\\")\\n\",\n",
    "    \"    check_func(df)\"\n",
    "   ],\n",
    "   \"id\": \"81d7f3d6134039c5\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": [\n",
    "    \"### 2.3 EDA - Part 2: Visualizations\\n\",\n",
    "    \"1. Rating Profile\\n\",\n",
    "    \"2. Rating by Segments\\n\",\n",
    "    \"3. Response Time by Rating\"\n",
    "   ],\n",
    "   \"id\": \"a4f8050c3f05d5ae\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"# Baseline - Customer Rating Profile\\n\",\n",
    "    \"plt.figure(figsize=(10, 5))\\n\",\n",
    "    \"sns.countplot(data=df, x='customer_rating', color='#3498db')\\n\",\n",
    "    \"plt.title('CUSTOMER RATING PROFILE', fontsize=16, fontweight='bold')\\n\",\n",
    "    \"plt.xlabel('Customer Rating (1-5 Stars)', fontsize=12)\\n\",\n",
    "    \"plt.ylabel('Number of Reviews', fontsize=12)\\n\",\n",
    "    \"plt.ylim(0, 6000)  # Extend upper limit for y_axis for better visibility\\n\",\n",
    "    \"plt.grid(axis='y', alpha=0.3, linestyle='--', linewidth=0.5)\\n\",\n",
    "    \"plt.xticks(fontsize=10)\\n\",\n",
    "    \"plt.yticks(fontsize=10)\\n\",\n",
    "    \"plt.gca().set_axisbelow(True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add value labels on top of bars\\n\",\n",
    "    \"rating_counts = df['customer_rating'].value_counts().sort_index()\\n\",\n",
    "    \"for i, v in enumerate(rating_counts.to_list()):\\n\",\n",
    "    \"    plt.text(i, v + 100, str(v), ha='center', fontweight\\n\",\n",
    "    \"              ='bold', fontsize=10)\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ],\n",
    "   \"id\": \"5ecc38db2cda327a\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"# Rating by Segments (product_category, platform, region, age_group)\\n\",\n",
    "    \"segment_attrs = ['product_category', 'platform', 'region', 'age_group']\\n\",\n",
    "    \"\\n\",\n",
    "    \"for attr in segment_attrs:\\n\",\n",
    "    \"    # Count unique values for this attribute\\n\",\n",
    "    \"    n_categories = df[attr].nunique()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Generate enough distinct colors\\n\",\n",
    "    \"    if n_categories <= 10:\\n\",\n",
    "    \"        palette = sns.color_palette('tab10', n_categories)\\n\",\n",
    "    \"    elif n_categories <= 20:\\n\",\n",
    "    \"        palette = sns.color_palette('tab20', n_categories)\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        # For 20+ categories, use husl which generates unlimited distinct colors\\n\",\n",
    "    \"        palette = sns.color_palette('husl', n_categories)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    plt.figure(figsize=(15, 5))\\n\",\n",
    "    \"    sns.countplot(data=df, x='customer_rating', hue=attr, palette=palette, order=[1, 2, 3, 4, 5])\\n\",\n",
    "    \"    plt.title(f\\\"CUSTOMER RATING BY {attr.upper().replace('_',' ')}\\\", fontsize=16, fontweight='bold')\\n\",\n",
    "    \"    plt.xlabel('Customer Rating (1-5 Stars)', fontsize=14)\\n\",\n",
    "    \"    plt.ylabel('Number of Reviews', fontsize=12)\\n\",\n",
    "    \"    plt.grid(axis='y', alpha=0.4, linestyle='--', linewidth=0.5)\\n\",\n",
    "    \"    plt.xticks(fontsize=12)\\n\",\n",
    "    \"    plt.yticks(fontsize=12)\\n\",\n",
    "    \"    plt.gca().set_axisbelow(True)\\n\",\n",
    "    \"    plt.legend(title=attr.replace('_', ' ').title(), fontsize=10, title_fontsize=12,\\n\",\n",
    "    \"               loc='center left', bbox_to_anchor=(1, 0.5))\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\"\n",
    "   ],\n",
    "   \"id\": \"7b5c8775073d8cc1\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"# Response Time Statistics by Rating\\n\",\n",
    "    \"stats = df.groupby('customer_rating')['response_time_hours'].agg(['min', 'mean', 'max'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig, ax = plt.subplots(figsize=(10, 5))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Bars configuration\\n\",\n",
    "    \"bar_config = [\\n\",\n",
    "    \"    (-0.25, 'min', 'Min'), # Left bar\\n\",\n",
    "    \"    (0, 'mean', 'Mean'),   # Center bar\\n\",\n",
    "    \"    (0.25, 'max', 'Max')   # Right bar\\n\",\n",
    "    \"]\\n\",\n",
    "    \"# Colors for the bars\\n\",\n",
    "    \"colors = sns.color_palette('deep6', 3)\\n\",\n",
    "    \"x = np.arange(1, 6)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Bars and annotations\\n\",\n",
    "    \"for (offset, col, label), color in zip(bar_config, colors):\\n\",\n",
    "    \"    values = stats[col].to_list()\\n\",\n",
    "    \"    ax.bar(x + offset, values, width=0.25, label=label, color=color, alpha=0.85)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Add value labels\\n\",\n",
    "    \"    for i, v in enumerate(values):\\n\",\n",
    "    \"        weight = 'bold' if col == 'mean' else 'normal'\\n\",\n",
    "    \"        ax.text(x[i] + offset, v + 1, f'{v:.1f}', ha='center',\\n\",\n",
    "    \"                fontsize=9, fontweight=weight)\\n\",\n",
    "    \"# Axes and grid configuration\\n\",\n",
    "    \"ax.set_title('RESPONSE TIME BY RATING â€” MIN, MEAN, MAX', fontsize=16, fontweight='bold')\\n\",\n",
    "    \"ax.set_xlabel('Customer Rating (1-5 Stars)', fontsize=12)\\n\",\n",
    "    \"ax.set_ylabel('Response Time (Hours)', fontsize=12)\\n\",\n",
    "    \"ax.set_xticks(x)\\n\",\n",
    "    \"ax.set_xlim(0.5, 5.5)\\n\",\n",
    "    \"ax.set_ylim(0, 80)\\n\",\n",
    "    \"ax.legend(fontsize=10, loc='center left', bbox_to_anchor=(1, 0.5))\\n\",\n",
    "    \"ax.grid(axis='y', alpha=0.3, linestyle='--', linewidth=0.5)\\n\",\n",
    "    \"ax.grid(axis='x', visible=False)\\n\",\n",
    "    \"ax.set_axisbelow(True)\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ],\n",
    "   \"id\": \"fa4e5b2595f92ba7\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"## Part 3: Text Preprocessing\",\n",
    "   \"id\": \"19e9a5e6db53f9c5\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"#### Preprocessing Functions\",\n",
    "   \"id\": \"d07e6d8d28a7f85c\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"# NLTK Lemmatizer Initialization\\n\",\n",
    "    \"lemmatizer = WordNetLemmatizer()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Text Cleaning Function\\n\",\n",
    "    \"def clean_text(text):\\n\",\n",
    "    \"    \\\"\\\"\\\" Clean and Normalize review text \\\"\\\"\\\"\\n\",\n",
    "    \"    # Lowercase\\n\",\n",
    "    \"    text = text.lower()\\n\",\n",
    "    \"    # Remove URLs, we don't really have in our dataset, but just in case\\n\",\n",
    "    \"    text = re.sub(r'http\\\\S+|www\\\\S+|https\\\\S+', '', text, flags=re.MULTILINE)\\n\",\n",
    "    \"    # Remove HTML tags, we don't really have in our dataset, but just in case\\n\",\n",
    "    \"    text = re.sub(r'<.*?>', '', text)\\n\",\n",
    "    \"    # remove email addresses, we don't really have in our dataset, but just in case\\n\",\n",
    "    \"    text = re.sub(r'\\\\S+@\\\\S+', '', text)\\n\",\n",
    "    \"    # Remove punctuation\\n\",\n",
    "    \"    text = text.translate(str.maketrans('', '', string.punctuation))\\n\",\n",
    "    \"    # Remove white spaces, newlines, tabs\\n\",\n",
    "    \"    text = re.sub(r'\\\\s+', ' ', text).strip()\\n\",\n",
    "    \"    return text\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Tokenization\\n\",\n",
    "    \"def tokenize_text(text):\\n\",\n",
    "    \"    \\\"\\\"\\\"Tokenize text into words.\\\"\\\"\\\"\\n\",\n",
    "    \"    return word_tokenize(text)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Remove stopwords\\n\",\n",
    "    \"def remove_stopwords(tokens):\\n\",\n",
    "    \"    \\\"\\\"\\\"Remove stopwords from token list.\\\"\\\"\\\"\\n\",\n",
    "    \"    stop_words = set(stopwords.words('english'))\\n\",\n",
    "    \"    negations = {\\\"not\\\", \\\"no\\\", \\\"n't\\\", \\\"don't\\\", \\\"doesn't\\\", \\\"didn't\\\",\\n\",\n",
    "    \"                 \\\"never\\\", \\\"none\\\", \\\"nobody\\\", \\\"nothing\\\", \\\"neither\\\", \\\"nor\\\"}\\n\",\n",
    "    \"    return [word for word in tokens if word not in stop_words or word in negations]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Lemmatization\\n\",\n",
    "    \"def lemmatize_tokens(tokens):\\n\",\n",
    "    \"    \\\"\\\"\\\"Lemmatize tokens using NLTK WordNetLemmatizer.\\\"\\\"\\\"\\n\",\n",
    "    \"    return [lemmatizer.lemmatize(token) for token in tokens]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Full Preprocessing Pipeline\\n\",\n",
    "    \"def preprocess_text(text):\\n\",\n",
    "    \"    \\\"\\\"\\\"Full text preprocessing pipeline.\\\"\\\"\\\"\\n\",\n",
    "    \"    text = clean_text(text)\\n\",\n",
    "    \"    tokens = tokenize_text(text)\\n\",\n",
    "    \"    tokens = remove_stopwords(tokens)\\n\",\n",
    "    \"    tokens = lemmatize_tokens(tokens)\\n\",\n",
    "    \"    return ' '.join(tokens)\\n\",\n",
    "    \"print(\\\"Text preprocessing functions ready!\\\")\"\n",
    "   ],\n",
    "   \"id\": \"83dca14962653f6e\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"#### Create Sentiment Labels\",\n",
    "   \"id\": \"fadb60b1a261f6d7\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"# Map customer ratings to sentiment labels\\n\",\n",
    "    \"def classify_sentiment(rating):\\n\",\n",
    "    \"    \\\"\\\"\\\"Classify sentiment based on customer rating.\\\"\\\"\\\"\\n\",\n",
    "    \"    if rating <= 2:\\n\",\n",
    "    \"        return 'negative'\\n\",\n",
    "    \"    elif rating == 3:\\n\",\n",
    "    \"        return 'neutral'\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        return 'positive'\\n\",\n",
    "    \"# Apply sentiment classification\\n\",\n",
    "    \"df['sentiment_label'] = df['customer_rating'].apply(classify_sentiment)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display sentiment label distribution after mapping to verify\\n\",\n",
    "    \"print(\\\"\\\\nSentiment Label Distribution After Mapping from (ratings):\\\")\\n\",\n",
    "    \"print(df['sentiment_label'].value_counts())\\n\",\n",
    "    \"print(\\\"\\\\nPercentages:\\\",)\\n\",\n",
    "    \"print((df['sentiment_label'].value_counts(normalize=True) * 100).round(1))\"\n",
    "   ],\n",
    "   \"id\": \"400ff7642988f7f6\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"#### Apply Preprocessing to the Reviews\",\n",
    "   \"id\": \"e8451608b6f2c451\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"print(\\\"Preprocessing review texts...\\\")\\n\",\n",
    "    \"df['processed_text'] = df['review_text'].apply(preprocess_text)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display some sample processed reviews\\n\",\n",
    "    \"print(\\\"\\\\nSample Processed Reviews:\\\")\\n\",\n",
    "    \"for i in range(5):\\n\",\n",
    "    \"    print(f\\\"\\\\nOriginal Review : {df.loc[i, 'review_text']}\\\")\\n\",\n",
    "    \"    print(f\\\"Processed Review: {df.loc[i, 'processed_text']}\\\")\\n\",\n",
    "    \"    print(f\\\"Sentiment Label : {df.loc[i, 'sentiment_label']}\\\")\"\n",
    "   ],\n",
    "   \"id\": \"7ead7a16c695a965\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"## Part 4: Feature Extraction\",\n",
    "   \"id\": \"b4f8d3a2bd03795d\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"####  Train-Test Split\",\n",
    "   \"id\": \"5f1f88bdab35063\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"# Split data into training and testing sets (80-20 split)\\n\",\n",
    "    \"X = df['processed_text']\\n\",\n",
    "    \"y = df['sentiment_label']\\n\",\n",
    "    \"\\n\",\n",
    "    \"X_train, X_test, y_train, y_test = train_test_split(\\n\",\n",
    "    \"    X, y,\\n\",\n",
    "    \"    test_size=0.2,\\n\",\n",
    "    \"    random_state=42,\\n\",\n",
    "    \"    stratify=y)\\n\",\n",
    "    \"print(f\\\"Training set:{len(X_train):,} reviews\\\")\\n\",\n",
    "    \"print(f\\\"Testing set :{len(X_test):,} reviews\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nSentiment Distribution in Training Set:\\\")\\n\",\n",
    "    \"print(y_train.value_counts())\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nSentiment Distribution in Testing Set:\\\")\\n\",\n",
    "    \"print(y_test.value_counts())\"\n",
    "   ],\n",
    "   \"id\": \"152d9b265fcd72a6\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"source\": \"#### TF-IDF Vectorization\",\n",
    "   \"id\": \"3bbf4219961e9639\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"# Initialize TF-IDF Vectorizer\\n\",\n",
    "    \"tfidf = TfidfVectorizer(\\n\",\n",
    "    \"    max_features=5000,  # Limit to top 5000 features\\n\",\n",
    "    \"    ngram_range=(1, 2), # Unigrams + Bigrams to capture something like \\\"not good\\\", \\\"very disappointing\\\"\\n\",\n",
    "    \"    min_df = 10, # Ignore terms that appear in less than 10 documents\\n\",\n",
    "    \"    max_df = 0.9  # Ignore terms that appear in more than 90% of documents\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Fit on training data and transform both training and testing data\\n\",\n",
    "    \"X_train_tfidf = tfidf.fit_transform(X_train)\\n\",\n",
    "    \"X_test_tfidf = tfidf.transform(X_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"TF-IDF matrix shape (train): {X_train_tfidf.shape}\\\")\\n\",\n",
    "    \"print(f\\\"TF-IDF matrix shape (test): {X_test_tfidf.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Vocabulary size: {len(tfidf.vocabulary_):,} features\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display top features\\n\",\n",
    "    \"feature_names = tfidf.get_feature_names_out()\\n\",\n",
    "    \"print(f\\\"\\\\nSample features (first 20):\\\")\\n\",\n",
    "    \"print(list(feature_names[:20]))\"\n",
    "   ],\n",
    "   \"id\": \"e789d0be8f6f9a16\"\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.13.9\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ],
   "id": "3fc25386ad723423"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
